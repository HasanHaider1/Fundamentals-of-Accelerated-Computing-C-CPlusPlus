{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><div align=\"center\">Asynchronous Streaming, and Visual Profiling with CUDA C/C++</div></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![CUDA](./images/CUDA_Logo.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The CUDA tookit ships with the **Nsight Systems**, a powerful GUI application to support the development of accelerated CUDA applications. Nsight Systems generates a graphical timeline of an accelerated application, with detailed information about CUDA API calls, kernel execution, memory activity, and the use of **CUDA streams**.\n",
    "\n",
    "In this lab, you will be using the Nsight Systems timeline to guide you in optimizing accelerated applications. Additionally, you will learn some intermediate CUDA programming techniques to support your work: **unmanaged memory allocation and migration**; **pinning**, or **page-locking** host memory; and **non-default concurrent CUDA streams**.\n",
    "\n",
    "At the end of this lab, you will be presented with an assessment, to accelerate and optimize a simple n-body particle simulator, which will allow you to demonstrate the skills you have developed during this course. Those of you who are able to accelerate the simulator while maintaining its correctness, will be granted a certification as proof of your competency."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Prerequisites\n",
    "\n",
    "To get the most out of this lab you should already be able to:\n",
    "\n",
    "- Write, compile, and run C/C++ programs that both call CPU functions and launch GPU kernels.\n",
    "- Control parallel thread hierarchy using execution configuration.\n",
    "- Refactor serial loops to execute their iterations in parallel on a GPU.\n",
    "- Allocate and free CUDA Unified Memory.\n",
    "- Understand the behavior of Unified Memory with regard to page faulting and data migrations.\n",
    "- Use asynchronous memory prefetching to reduce page faults and data migrations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objectives\n",
    "\n",
    "By the time you complete this lab you will be able to:\n",
    "\n",
    "- Use **Nsight Systems** to visually profile the timeline of GPU-accelerated CUDA applications.\n",
    "- Use Nsight Systems to identify, and exploit, optimization opportunities in GPU-accelerated CUDA applications.\n",
    "- Utilize CUDA streams for concurrent kernel execution in accelerated applications.\n",
    "- (**Optional Advanced Content**) Use manual device memory allocation, including allocating pinned memory, in order to asynchronously transfer data in concurrent CUDA streams."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Running Nsight Systems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this interactive lab environment, we have set up a remote desktop you can access from your browser, where you will be able to launch and use Nsight Systems.\n",
    "\n",
    "You will begin by creating a report file for an already-existing vector addition program, after which you will be walked through a series of steps to open this report file in Nsight Systems, and to make the visual experience nice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Report File\n",
    "\n",
    "[`01-vector-add.cu`](../edit/01-vector-add/01-vector-add.cu) (<-------- click on these links to source files to edit them in the browser) contains a working, accelerated, vector addition application. Use the code execution cell directly below (you can execute it, and any of the code execution cells in this lab by `CTRL` + clicking it) to compile and run it. You should see a message printed that indicates it was successful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success! All values calculated correctly.\r\n"
     ]
    }
   ],
   "source": [
    "!nvcc -o vector-add-no-prefetch 01-vector-add/01-vector-add.cu -run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, use `nsys profile --stats=true` to create a report file that you will be able to open in the Nsight Systems visual profiler. Here we use the `-o` flag to give the report file a memorable name:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "**** collection configuration ****\n",
      "\toutput_filename = /dli/task/vector-add-no-prefetch-report\n",
      "\tforce-overwrite = false\n",
      "\tstop-on-exit = true\n",
      "\texport_sqlite = true\n",
      "\tstats = true\n",
      "\tcapture-range = none\n",
      "\tstop-on-range-end = false\n",
      "\tBeta: ftrace events:\n",
      "\tftrace-keep-user-config = false\n",
      "\ttrace-GPU-context-switch = false\n",
      "\tdelay = 0 seconds\n",
      "\tduration = 0 seconds\n",
      "\tkill = signal number 15\n",
      "\tinherit-environment = true\n",
      "\tshow-output = true\n",
      "\ttrace-fork-before-exec = false\n",
      "\tsample_cpu = true\n",
      "\tbacktrace_method = LBR\n",
      "\twait = all\n",
      "\ttrace_cublas = false\n",
      "\ttrace_cuda = true\n",
      "\ttrace_cudnn = false\n",
      "\ttrace_nvtx = true\n",
      "\ttrace_mpi = false\n",
      "\ttrace_openacc = false\n",
      "\ttrace_vulkan = false\n",
      "\ttrace_opengl = true\n",
      "\ttrace_osrt = true\n",
      "\tosrt-threshold = 0 nanoseconds\n",
      "\tcudabacktrace = false\n",
      "\tcudabacktrace-threshold = 0 nanoseconds\n",
      "\tprofile_processes = tree\n",
      "\tapplication command = ./vector-add-no-prefetch\n",
      "\tapplication arguments = \n",
      "\tapplication working directory = /dli/task\n",
      "\tNVTX profiler range trigger = \n",
      "\tNVTX profiler domain trigger = \n",
      "\tenvironment variables:\n",
      "\tCollecting data...\n",
      "Success! All values calculated correctly.\n",
      "\tGenerating the /dli/task/vector-add-no-prefetch-report.qdstrm file.\n",
      "\tCapturing raw events...\n",
      "\t13667 total events collected.\n",
      "\tSaving diagnostics...\n",
      "\tSaving qdstrm file to disk...\n",
      "\tFinished saving file.\n",
      "\n",
      "\n",
      "Importing the qdstrm file using /opt/nvidia/nsight-systems/2019.5.2/host-linux-x64/QdstrmImporter.\n",
      "\n",
      "Importing...\n",
      "\n",
      "Importing [==================================================100%]\n",
      "Saving report to file \"/dli/task/vector-add-no-prefetch-report.qdrep\"\n",
      "Report file saved.\n",
      "Please discard the qdstrm file and use the qdrep file instead.\n",
      "\n",
      "Removed /dli/task/vector-add-no-prefetch-report.qdstrm as it was successfully imported.\n",
      "Please use the qdrep file instead.\n",
      "\n",
      "Exporting the qdrep file to SQLite database using /opt/nvidia/nsight-systems/2019.5.2/host-linux-x64/nsys-exporter.\n",
      "\n",
      "Exporting 13625 events:\n",
      "\n",
      "0%   10   20   30   40   50   60   70   80   90   100%\n",
      "|----|----|----|----|----|----|----|----|----|----|\n",
      "***************************************************\n",
      "\n",
      "Exported successfully to\n",
      "/dli/task/vector-add-no-prefetch-report.sqlite\n",
      "\n",
      "Generating CUDA API Statistics...\n",
      "CUDA API Statistics (nanoseconds)\n",
      "\n",
      "Time(%)      Total Time       Calls         Average         Minimum         Maximum  Name                                                                            \n",
      "-------  --------------  ----------  --------------  --------------  --------------  --------------------------------------------------------------------------------\n",
      "   63.4       229029745           3      76343248.3           26482       228910975  cudaMallocManaged                                                               \n",
      "   29.8       107687724           1     107687724.0       107687724       107687724  cudaDeviceSynchronize                                                           \n",
      "    6.8        24543371           3       8181123.7         7593907         9270731  cudaFree                                                                        \n",
      "    0.0           69552           1         69552.0           69552           69552  cudaLaunchKernel                                                                \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Generating CUDA Kernel Statistics...\n",
      "\n",
      "Generating CUDA Memory Operation Statistics...\n",
      "CUDA Kernel Statistics (nanoseconds)\n",
      "\n",
      "Time(%)      Total Time   Instances         Average         Minimum         Maximum  Name                                                                            \n",
      "-------  --------------  ----------  --------------  --------------  --------------  --------------------------------------------------------------------------------\n",
      "  100.0       107677341           1     107677341.0       107677341       107677341  addVectorsInto                                                                  \n",
      "\n",
      "\n",
      "CUDA Memory Operation Statistics (nanoseconds)\n",
      "\n",
      "Time(%)      Total Time  Operations         Average         Minimum         Maximum  Name                                                                            \n",
      "-------  --------------  ----------  --------------  --------------  --------------  --------------------------------------------------------------------------------\n",
      "   85.1        70548288       11570          6097.5            2816          158976  [CUDA Unified Memory memcpy HtoD]                                               \n",
      "   14.9        12329504         768         16054.0            1824          110976  [CUDA Unified Memory memcpy DtoH]                                               \n",
      "\n",
      "\n",
      "CUDA Memory Operation Statistics (KiB)\n",
      "\n",
      "            Total      Operations            Average            Minimum            Maximum  Name                                                                            \n",
      "-----------------  --------------  -----------------  -----------------  -----------------  --------------------------------------------------------------------------------\n",
      "         393216.0           11570               34.0              4.000             1012.0  [CUDA Unified Memory memcpy HtoD]                                               \n",
      "         131072.0             768              170.7              4.000             1020.0  [CUDA Unified Memory memcpy DtoH]                                               \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Generating Operating System Runtime API Statistics...\n",
      "Operating System Runtime API Statistics (nanoseconds)\n",
      "\n",
      "Time(%)      Total Time       Calls         Average         Minimum         Maximum  Name                                                                            \n",
      "-------  --------------  ----------  --------------  --------------  --------------  --------------------------------------------------------------------------------\n",
      "   54.3      1530294933          82      18662133.3           45327       100226465  poll                                                                            \n",
      "   41.5      1169025494          84      13916970.2           24994       100135988  sem_timedwait                                                                   \n",
      "    3.1        87811607         581        151138.7            1120        18931657  ioctl                                                                           \n",
      "    1.0        27602500          87        317270.1            1982         9162784  mmap                                                                            \n",
      "    0.0          942959          96          9822.5            1822           38141  fopen                                                                           \n",
      "    0.0          630703          78          8085.9            4071           13967  open64                                                                          \n",
      "    0.0          310426          89          3487.9            1600            7456  fclose                                                                          \n",
      "    0.0          211914           4         52978.5           41284           59919  pthread_create                                                                  \n",
      "    0.0          129686          10         12968.6            8686           17312  write                                                                           \n",
      "    0.0          118049           3         39349.7           35093           42583  fgets                                                                           \n",
      "    0.0          112339          81          1386.9            1022            6622  fcntl                                                                           \n",
      "    0.0           59933          11          5448.5            2213            8528  munmap                                                                          \n",
      "    0.0           47297           5          9459.4            4774           12158  open                                                                            \n",
      "    0.0           27364          12          2280.3            1338            4114  read                                                                            \n",
      "    0.0           23174           3          7724.7            7225            8564  pipe2                                                                           \n",
      "    0.0           22360           2         11180.0           10395           11965  socket                                                                          \n",
      "    0.0           14115           4          3528.8            2114            4762  mprotect                                                                        \n",
      "    0.0           11725           1         11725.0           11725           11725  connect                                                                         \n",
      "    0.0            9018           2          4509.0            4456            4562  fread                                                                           \n",
      "    0.0            6738           1          6738.0            6738            6738  pthread_cond_broadcast                                                          \n",
      "    0.0            4907           1          4907.0            4907            4907  bind                                                                            \n",
      "    0.0            3212           1          3212.0            3212            3212  listen                                                                          \n",
      "\n",
      "\n",
      "\n",
      "\r\n",
      "Generating NVTX Push-Pop Range Statistics...\r\n",
      "NVTX Push-Pop Range Statistics (nanoseconds)\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "!nsys profile --stats=true -o vector-add-no-prefetch-report ./vector-add-no-prefetch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Open the Remote Desktop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the next cell to generate a link to a remote desktop then read the instructions that follow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "var url = window.location.hostname + ':5901';\n",
       "element.append('<a href=\"'+url+'\">Open Remote Desktop</a>')\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%js\n",
    "var url = window.location.hostname + ':5901';\n",
    "element.append('<a href=\"'+url+'\">Open Remote Desktop</a>')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After clicking the _Connect_ button you will be asked for a password, which is `nvidia`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Open the Remote Desktop Terminal Application"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, click on the icon for the terminal application, found at the bottom of the screen of the remote desktop:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![terminal](images/terminal.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Open Nsight Systems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To open Nsight Systems, enter and run the `nsight-sys` command from the now-open terminal:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![open nsight](images/open-nsight.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Enable Usage Reporting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When prompted, click \"Yes\" to enable usage reporting:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![enable usage](images/enable_usage.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Open the Report File"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Open this report file by visiting _File_ -> _Open_ from the Nsight Systems menu, then go to the path `/root/Desktop/reports` and select `vector-add-no-prefetch-report.qdrep`. All the reports you generate in this lab will be in this `root/Desktop/reports` directory:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![open-report](images/open-report.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ignore Warnings/Errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can close and ignore any warnings or errors you see, which are just a result of our particular remote desktop environment:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![ignore errors](images/ignore-error.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make More Room for the Timelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make your experience nicer, full-screen the profiler, close the _Project Explorer_ and hide the *Events View*:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![make nice](images/make-nice.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your screen should now look like this:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![now nice](images/now-nice.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expand the CUDA Unified Memory Timelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, expand the _CUDA_ -> _Unified memory_ and _Context_ timelines, and close the _OS runtime libraries_ timelines:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![open memory](images/open-memory.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observe Many Memory Transfers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From a glance you can see that your application is taking about 1 second to run, and that also, during the time when the `addVectorsInto` kernel is running, that there is a lot of UM memory activity:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![memory and kernel](images/memory-and-kernel.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zoom into the memory timelines to see more clearly all the small memory transfers being caused by the on-demand memory page faults. A couple tips:\n",
    "\n",
    "1. You can zoom in and out at any point of the timeline by holding `Ctrl` while scrolling your mouse/trackpad\n",
    "2. You can zoom into any section by click + dragging a rectangle around it, and then selecting _Zoom in_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's an example of zooming in to see the many small memory transfers:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![many transfers](images/many-transfers.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Comparing Code Refactors Iteratively with Nsight Systems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you have Nsight Systems up and running and are comfortable moving around the timelines, you will be profiling a series of programs that were iteratively improved using techniques already familiar to you. Each time you profile, information in the timeline will give information supporting how you should next modify your code. Doing this will further increase your understanding of how various CUDA programming techniques affect application performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Compare the Timelines of Prefetching vs. Non-Prefetching\n",
    "\n",
    "[`01-vector-add-prefetch-solution.cu`](../edit/01-vector-add/solutions/01-vector-add-prefetch-solution.cu) refactors the vector addition application from above so that the 3 vectors needed by its `addVectorsInto` kernel are asynchronously prefetched to the active GPU device prior to launching the kernel (using [`cudaMemPrefetchAsync`](http://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__MEMORY.html#group__CUDART__MEMORY_1ge8dc9199943d421bc8bc7f473df12e42)). Open the source code and identify where in the application these changes were made.\n",
    "\n",
    "After reviewing the changes, compile and run the refactored application using the code execution cell directly below. You should see its success message printed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success! All values calculated correctly.\r\n"
     ]
    }
   ],
   "source": [
    "!nvcc -o vector-add-prefetch 01-vector-add/solutions/01-vector-add-prefetch-solution.cu -run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now create a report file for this version of the application:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "**** collection configuration ****\n",
      "\toutput_filename = /dli/task/vector-add-prefetch-report\n",
      "\tforce-overwrite = false\n",
      "\tstop-on-exit = true\n",
      "\texport_sqlite = true\n",
      "\tstats = true\n",
      "\tcapture-range = none\n",
      "\tstop-on-range-end = false\n",
      "\tBeta: ftrace events:\n",
      "\tftrace-keep-user-config = false\n",
      "\ttrace-GPU-context-switch = false\n",
      "\tdelay = 0 seconds\n",
      "\tduration = 0 seconds\n",
      "\tkill = signal number 15\n",
      "\tinherit-environment = true\n",
      "\tshow-output = true\n",
      "\ttrace-fork-before-exec = false\n",
      "\tsample_cpu = true\n",
      "\tbacktrace_method = LBR\n",
      "\twait = all\n",
      "\ttrace_cublas = false\n",
      "\ttrace_cuda = true\n",
      "\ttrace_cudnn = false\n",
      "\ttrace_nvtx = true\n",
      "\ttrace_mpi = false\n",
      "\ttrace_openacc = false\n",
      "\ttrace_vulkan = false\n",
      "\ttrace_opengl = true\n",
      "\ttrace_osrt = true\n",
      "\tosrt-threshold = 0 nanoseconds\n",
      "\tcudabacktrace = false\n",
      "\tcudabacktrace-threshold = 0 nanoseconds\n",
      "\tprofile_processes = tree\n",
      "\tapplication command = ./vector-add-prefetch\n",
      "\tapplication arguments = \n",
      "\tapplication working directory = /dli/task\n",
      "\tNVTX profiler range trigger = \n",
      "\tNVTX profiler domain trigger = \n",
      "\tenvironment variables:\n",
      "\tCollecting data...\n",
      "Success! All values calculated correctly.\n",
      "\tGenerating the /dli/task/vector-add-prefetch-report.qdstrm file.\n",
      "\tCapturing raw events...\n",
      "\t2293 total events collected.\n",
      "\tSaving diagnostics...\n",
      "\tSaving qdstrm file to disk...\n",
      "\tFinished saving file.\n",
      "\n",
      "\n",
      "Importing the qdstrm file using /opt/nvidia/nsight-systems/2019.5.2/host-linux-x64/QdstrmImporter.\n",
      "\n",
      "Importing...\n",
      "\n",
      "Importing [==================================================100%]\n",
      "Saving report to file \"/dli/task/vector-add-prefetch-report.qdrep\"\n",
      "Report file saved.\n",
      "Please discard the qdstrm file and use the qdrep file instead.\n",
      "\n",
      "Removed /dli/task/vector-add-prefetch-report.qdstrm as it was successfully imported.\n",
      "Please use the qdrep file instead.\n",
      "\n",
      "Exporting the qdrep file to SQLite database using /opt/nvidia/nsight-systems/2019.5.2/host-linux-x64/nsys-exporter.\n",
      "\n",
      "Exporting 2251 events:\n",
      "\n",
      "0%   10   20   30   40   50   60   70   80   90   100%\n",
      "|----|----|----|----|----|----|----|----|----|----|\n",
      "***************************************************\n",
      "\n",
      "Exported successfully to\n",
      "/dli/task/vector-add-prefetch-report.sqlite\n",
      "\n",
      "Generating CUDA API Statistics...\n",
      "CUDA API Statistics (nanoseconds)\n",
      "\n",
      "Time(%)      Total Time       Calls         Average         Minimum         Maximum  Name                                                                            \n",
      "-------  --------------  ----------  --------------  --------------  --------------  --------------------------------------------------------------------------------\n",
      "   76.0       202301371           3      67433790.3           26329       202214811  cudaMallocManaged                                                               \n",
      "   11.2        29829004           1      29829004.0        29829004        29829004  cudaDeviceSynchronize                                                           \n",
      "    8.7        23055973           3       7685324.3         6817360         9013314  cudaFree                                                                        \n",
      "    4.1        10893078           3       3631026.0            9112        10263449  cudaMemPrefetchAsync                                                            \n",
      "    0.0           45847           1         45847.0           45847           45847  cudaLaunchKernel                                                                \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Generating CUDA Kernel Statistics...\n",
      "\n",
      "Generating CUDA Memory Operation Statistics...\n",
      "CUDA Kernel Statistics (nanoseconds)\n",
      "\n",
      "Time(%)      Total Time   Instances         Average         Minimum         Maximum  Name                                                                            \n",
      "-------  --------------  ----------  --------------  --------------  --------------  --------------------------------------------------------------------------------\n",
      "  100.0          506717           1        506717.0          506717          506717  addVectorsInto                                                                  \n",
      "\n",
      "\n",
      "CUDA Memory Operation Statistics (nanoseconds)\n",
      "\n",
      "Time(%)      Total Time  Operations         Average         Minimum         Maximum  Name                                                                            \n",
      "-------  --------------  ----------  --------------  --------------  --------------  --------------------------------------------------------------------------------\n",
      "   74.6        37083520         192        193143.3          192256          194688  [CUDA Unified Memory memcpy HtoD]                                               \n",
      "   25.4        12629280         768         16444.4            1920           99424  [CUDA Unified Memory memcpy DtoH]                                               \n",
      "\n",
      "\n",
      "CUDA Memory Operation Statistics (KiB)\n",
      "\n",
      "            Total      Operations            Average            Minimum            Maximum  Name                                                                            \n",
      "-----------------  --------------  -----------------  -----------------  -----------------  --------------------------------------------------------------------------------\n",
      "         393216.0             192             2048.0           2048.000             2048.0  [CUDA Unified Memory memcpy HtoD]                                               \n",
      "         131072.0             768              170.7              4.000             1020.0  [CUDA Unified Memory memcpy DtoH]                                               \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Generating Operating System Runtime API Statistics...\n",
      "Operating System Runtime API Statistics (nanoseconds)\n",
      "\n",
      "Time(%)      Total Time       Calls         Average         Minimum         Maximum  Name                                                                            \n",
      "-------  --------------  ----------  --------------  --------------  --------------  --------------------------------------------------------------------------------\n",
      "   53.4      1379298578          81      17028377.5            3159       100197858  poll                                                                            \n",
      "   41.2      1063121267          75      14174950.2           24427       100141801  sem_timedwait                                                                   \n",
      "    4.2       108931505         577        188789.4            1036        15829072  ioctl                                                                           \n",
      "    1.0        25450455          88        289209.7            1814         8959565  mmap                                                                            \n",
      "    0.1         2770426           3        923475.3           38108         2536784  sem_wait                                                                        \n",
      "    0.0          709190          96          7387.4            1775           21876  fopen                                                                           \n",
      "    0.0          685497           5        137099.4           32808          508101  pthread_create                                                                  \n",
      "    0.0          550989          78          7064.0            4639           15996  open64                                                                          \n",
      "    0.0          261874          89          2942.4            1597            6225  fclose                                                                          \n",
      "    0.0          164111          14         11722.2            1779           19684  write                                                                           \n",
      "    0.0           96659           3         32219.7           24653           42569  fgets                                                                           \n",
      "    0.0           92237          80          1153.0            1009            5398  fcntl                                                                           \n",
      "    0.0           45087          12          3757.3            2116            7623  munmap                                                                          \n",
      "    0.0           43350          16          2709.4            1347            5043  read                                                                            \n",
      "    0.0           40542           5          8108.4            4480           10434  open                                                                            \n",
      "    0.0           19183           2          9591.5            7064           12119  socket                                                                          \n",
      "    0.0           18923           3          6307.7            5623            7018  pipe2                                                                           \n",
      "    0.0           14996           5          2999.2            2082            3757  mprotect                                                                        \n",
      "    0.0           10770           1         10770.0           10770           10770  connect                                                                         \n",
      "    0.0            7586           1          7586.0            7586            7586  pthread_cond_broadcast                                                          \n",
      "    0.0            6883           2          3441.5            2791            4092  fread                                                                           \n",
      "    0.0            4024           1          4024.0            4024            4024  bind                                                                            \n",
      "    0.0            2636           1          2636.0            2636            2636  listen                                                                          \n",
      "    0.0            1070           1          1070.0            1070            1070  pthread_mutex_trylock                                                           \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Generating NVTX Push-Pop Range Statistics...\n",
      "NVTX Push-Pop Range Statistics (nanoseconds)\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!nsys profile --stats=true -o vector-add-prefetch-report ./vector-add-prefetch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Open the report in Nsight Systems, leaving the previous report open for comparison.\n",
    "\n",
    "- How does the execution time compare to that of the `addVectorsInto` kernel prior to adding asynchronous prefetching?\n",
    "- Locate `cudaMemPrefetchAsync` in the *CUDA API* section of the timeline.\n",
    "- How have the memory transfers changed?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Exercise: Profile Refactor with Launch Init in Kernel\n",
    "\n",
    "In the previous iteration of the vector addition application, the vector data is being initialized on the CPU, and therefore needs to be migrated to the GPU before the `addVectorsInto` kernel can operate on it.\n",
    "\n",
    "The next iteration of the application, [01-init-kernel-solution.cu](../edit/02-init-kernel/solutions/01-init-kernel-solution.cu), the application has been refactored to initialize the data in parallel on the GPU.\n",
    "\n",
    "Since the initialization now takes place on the GPU, prefetching has been done prior to initialization, rather than prior to the vector addition work. Review the source code to identify where these changes have been made.\n",
    "\n",
    "After reviewing the changes, compile and run the refactored application using the code execution cell directly below. You should see its success message printed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success! All values calculated correctly.\r\n"
     ]
    }
   ],
   "source": [
    "!nvcc -o init-kernel 02-init-kernel/solutions/01-init-kernel-solution.cu -run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now create a report file for this version of the application:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "**** collection configuration ****\n",
      "\toutput_filename = /dli/task/init-kernel-report\n",
      "\tforce-overwrite = false\n",
      "\tstop-on-exit = true\n",
      "\texport_sqlite = true\n",
      "\tstats = true\n",
      "\tcapture-range = none\n",
      "\tstop-on-range-end = false\n",
      "\tBeta: ftrace events:\n",
      "\tftrace-keep-user-config = false\n",
      "\ttrace-GPU-context-switch = false\n",
      "\tdelay = 0 seconds\n",
      "\tduration = 0 seconds\n",
      "\tkill = signal number 15\n",
      "\tinherit-environment = true\n",
      "\tshow-output = true\n",
      "\ttrace-fork-before-exec = false\n",
      "\tsample_cpu = true\n",
      "\tbacktrace_method = LBR\n",
      "\twait = all\n",
      "\ttrace_cublas = false\n",
      "\ttrace_cuda = true\n",
      "\ttrace_cudnn = false\n",
      "\ttrace_nvtx = true\n",
      "\ttrace_mpi = false\n",
      "\ttrace_openacc = false\n",
      "\ttrace_vulkan = false\n",
      "\ttrace_opengl = true\n",
      "\ttrace_osrt = true\n",
      "\tosrt-threshold = 0 nanoseconds\n",
      "\tcudabacktrace = false\n",
      "\tcudabacktrace-threshold = 0 nanoseconds\n",
      "\tprofile_processes = tree\n",
      "\tapplication command = ./init-kernel\n",
      "\tapplication arguments = \n",
      "\tapplication working directory = /dli/task\n",
      "\tNVTX profiler range trigger = \n",
      "\tNVTX profiler domain trigger = \n",
      "\tenvironment variables:\n",
      "\tCollecting data...\n",
      "Success! All values calculated correctly.\n",
      "\tGenerating the /dli/task/init-kernel-report.qdstrm file.\n",
      "\tCapturing raw events...\n",
      "\t2016 total events collected.\n",
      "\tSaving diagnostics...\n",
      "\tSaving qdstrm file to disk...\n",
      "\tFinished saving file.\n",
      "\n",
      "\n",
      "Importing the qdstrm file using /opt/nvidia/nsight-systems/2019.5.2/host-linux-x64/QdstrmImporter.\n",
      "\n",
      "Importing...\n",
      "\n",
      "Importing [==================================================100%]\n",
      "Saving report to file \"/dli/task/init-kernel-report.qdrep\"\n",
      "Report file saved.\n",
      "Please discard the qdstrm file and use the qdrep file instead.\n",
      "\n",
      "Removed /dli/task/init-kernel-report.qdstrm as it was successfully imported.\n",
      "Please use the qdrep file instead.\n",
      "\n",
      "Exporting the qdrep file to SQLite database using /opt/nvidia/nsight-systems/2019.5.2/host-linux-x64/nsys-exporter.\n",
      "\n",
      "Exporting 1979 events:\n",
      "\n",
      "0%   10   20   30   40   50   60   70   80   90   100%\n",
      "|----|----|----|----|----|----|----|----|----|----|\n",
      "***************************************************\n",
      "\n",
      "Exported successfully to\n",
      "/dli/task/init-kernel-report.sqlite\n",
      "\n",
      "Generating CUDA API Statistics...\n",
      "CUDA API Statistics (nanoseconds)\n",
      "\n",
      "Time(%)      Total Time       Calls         Average         Minimum         Maximum  Name                                                                            \n",
      "-------  --------------  ----------  --------------  --------------  --------------  --------------------------------------------------------------------------------\n",
      "   89.1       196678209           3      65559403.0           27002       196558262  cudaMallocManaged                                                               \n",
      "    8.9        19581827           3       6527275.7         5249162         8923817  cudaFree                                                                        \n",
      "    1.5         3271146           1       3271146.0         3271146         3271146  cudaDeviceSynchronize                                                           \n",
      "    0.5         1046503           3        348834.3           14531          875762  cudaMemPrefetchAsync                                                            \n",
      "    0.0           77635           4         19408.8            9952           45821  cudaLaunchKernel                                                                \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Generating CUDA Kernel Statistics...\n",
      "\n",
      "Generating CUDA Memory Operation Statistics...\n",
      "CUDA Kernel Statistics (nanoseconds)\n",
      "\n",
      "Time(%)      Total Time   Instances         Average         Minimum         Maximum  Name                                                                            \n",
      "-------  --------------  ----------  --------------  --------------  --------------  --------------------------------------------------------------------------------\n",
      "   51.6          503187           1        503187.0          503187          503187  addVectorsInto                                                                  \n",
      "   48.4          472052           3        157350.7          154684          161948  initWith                                                                        \n",
      "\n",
      "\n",
      "CUDA Memory Operation Statistics (nanoseconds)\n",
      "\n",
      "Time(%)      Total Time  Operations         Average         Minimum         Maximum  Name                                                                            \n",
      "-------  --------------  ----------  --------------  --------------  --------------  --------------------------------------------------------------------------------\n",
      "  100.0        12351104         768         16082.2            1920           99776  [CUDA Unified Memory memcpy DtoH]                                               \n",
      "\n",
      "\n",
      "CUDA Memory Operation Statistics (KiB)\n",
      "\n",
      "            Total      Operations            Average            Minimum            Maximum  Name                                                                            \n",
      "-----------------  --------------  -----------------  -----------------  -----------------  --------------------------------------------------------------------------------\n",
      "         131072.0             768              170.7              4.000             1020.0  [CUDA Unified Memory memcpy DtoH]                                               \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Generating Operating System Runtime API Statistics...\n",
      "Operating System Runtime API Statistics (nanoseconds)\n",
      "\n",
      "Time(%)      Total Time       Calls         Average         Minimum         Maximum  Name                                                                            \n",
      "-------  --------------  ----------  --------------  --------------  --------------  --------------------------------------------------------------------------------\n",
      "   45.4       476889786          30      15896326.2           23117       100141563  sem_timedwait                                                                   \n",
      "   45.0       472872574          35      13510645.0            2566       100193424  poll                                                                            \n",
      "    7.3        76703320         578        132704.7            1040        15983653  ioctl                                                                           \n",
      "    2.1        21925910          88        249158.1            1818         8844251  mmap                                                                            \n",
      "    0.1          683799          96          7122.9            1770           24732  fopen                                                                           \n",
      "    0.0          488364          78          6261.1            3927           12588  open64                                                                          \n",
      "    0.0          403914           3        134638.0           69516          214849  sem_wait                                                                        \n",
      "    0.0          255988          89          2876.3            1586            5506  fclose                                                                          \n",
      "    0.0          241272           5         48254.4           38114           55695  pthread_create                                                                  \n",
      "    0.0          143238          14         10231.3            1422           15058  write                                                                           \n",
      "    0.0           97750           3         32583.3           25852           41898  fgets                                                                           \n",
      "    0.0           93997          81          1160.5            1004            7137  fcntl                                                                           \n",
      "    0.0           52333          13          4025.6            1785            8861  munmap                                                                          \n",
      "    0.0           42705           5          8541.0            4841           10482  open                                                                            \n",
      "    0.0           36417          16          2276.1            1302            4357  read                                                                            \n",
      "    0.0           19864           2          9932.0            8245           11619  socket                                                                          \n",
      "    0.0           16957           3          5652.3            5008            6717  pipe2                                                                           \n",
      "    0.0           13624           5          2724.8            2247            3363  mprotect                                                                        \n",
      "    0.0           12135           3          4045.0            2360            5124  fread                                                                           \n",
      "    0.0           11769           1         11769.0           11769           11769  connect                                                                         \n",
      "    0.0            7189           2          3594.5            1043            6146  pthread_cond_broadcast                                                          \n",
      "    0.0            4283           1          4283.0            4283            4283  bind                                                                            \n",
      "    0.0            2410           1          2410.0            2410            2410  listen                                                                          \n",
      "    0.0            1177           1          1177.0            1177            1177  pthread_mutex_trylock                                                           \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Generating NVTX Push-Pop Range Statistics...\n",
      "NVTX Push-Pop Range Statistics (nanoseconds)\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!nsys profile --stats=true -o init-kernel-report ./init-kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Open the new report file in Nsight Systems and do the following:\n",
    "\n",
    "- Compare the application and `addVectorsInto` runtimes to the previous version of the application, how did they change?\n",
    "- Look at the *Kernels* section of the timeline. Which of the two kernels (`addVectorsInto` and the initialization kernel) is taking up the majority of the time on the GPU?\n",
    "- Which of the following does your application contain?\n",
    "  - Data Migration (HtoD)\n",
    "  - Data Migration (DtoH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Profile Refactor with Asynchronous Prefetch Back to the Host\n",
    "\n",
    "Currently, the vector addition application verifies the work of the vector addition kernel on the host. The next refactor of the application, [01-prefetch-check-solution.cu](../edit/04-prefetch-check/solutions/01-prefetch-check-solution.cu), asynchronously prefetches the data back to the host for verification.\n",
    "\n",
    "After reviewing the changes, compile and run the refactored application using the code execution cell directly below. You should see its success message printed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success! All values calculated correctly.\r\n"
     ]
    }
   ],
   "source": [
    "!nvcc -o prefetch-to-host 04-prefetch-check/solutions/01-prefetch-check-solution.cu -run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now create a report file for this version of the application:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "**** collection configuration ****\n",
      "\toutput_filename = /dli/task/prefetch-to-host-report\n",
      "\tforce-overwrite = false\n",
      "\tstop-on-exit = true\n",
      "\texport_sqlite = true\n",
      "\tstats = true\n",
      "\tcapture-range = none\n",
      "\tstop-on-range-end = false\n",
      "\tBeta: ftrace events:\n",
      "\tftrace-keep-user-config = false\n",
      "\ttrace-GPU-context-switch = false\n",
      "\tdelay = 0 seconds\n",
      "\tduration = 0 seconds\n",
      "\tkill = signal number 15\n",
      "\tinherit-environment = true\n",
      "\tshow-output = true\n",
      "\ttrace-fork-before-exec = false\n",
      "\tsample_cpu = true\n",
      "\tbacktrace_method = LBR\n",
      "\twait = all\n",
      "\ttrace_cublas = false\n",
      "\ttrace_cuda = true\n",
      "\ttrace_cudnn = false\n",
      "\ttrace_nvtx = true\n",
      "\ttrace_mpi = false\n",
      "\ttrace_openacc = false\n",
      "\ttrace_vulkan = false\n",
      "\ttrace_opengl = true\n",
      "\ttrace_osrt = true\n",
      "\tosrt-threshold = 0 nanoseconds\n",
      "\tcudabacktrace = false\n",
      "\tcudabacktrace-threshold = 0 nanoseconds\n",
      "\tprofile_processes = tree\n",
      "\tapplication command = ./prefetch-to-host\n",
      "\tapplication arguments = \n",
      "\tapplication working directory = /dli/task\n",
      "\tNVTX profiler range trigger = \n",
      "\tNVTX profiler domain trigger = \n",
      "\tenvironment variables:\n",
      "\tCollecting data...\n",
      "Success! All values calculated correctly.\n",
      "\tGenerating the /dli/task/prefetch-to-host-report.qdstrm file.\n",
      "\tCapturing raw events...\n",
      "\t1303 total events collected.\n",
      "\tSaving diagnostics...\n",
      "\tSaving qdstrm file to disk...\n",
      "\tFinished saving file.\n",
      "\n",
      "\n",
      "Importing the qdstrm file using /opt/nvidia/nsight-systems/2019.5.2/host-linux-x64/QdstrmImporter.\n",
      "\n",
      "Importing...\n",
      "\n",
      "Importing [==================================================100%]\n",
      "Saving report to file \"/dli/task/prefetch-to-host-report.qdrep\"\n",
      "Report file saved.\n",
      "Please discard the qdstrm file and use the qdrep file instead.\n",
      "\n",
      "Removed /dli/task/prefetch-to-host-report.qdstrm as it was successfully imported.\n",
      "Please use the qdrep file instead.\n",
      "\n",
      "Exporting the qdrep file to SQLite database using /opt/nvidia/nsight-systems/2019.5.2/host-linux-x64/nsys-exporter.\n",
      "\n",
      "Exporting 1266 events:\n",
      "\n",
      "0%   10   20   30   40   50   60   70   80   90   100%\n",
      "|----|----|----|----|----|----|----|----|----|----|\n",
      "***************************************************\n",
      "\n",
      "Exported successfully to\n",
      "/dli/task/prefetch-to-host-report.sqlite\n",
      "\n",
      "Generating CUDA API Statistics...\n",
      "CUDA API Statistics (nanoseconds)\n",
      "\n",
      "Time(%)      Total Time       Calls         Average         Minimum         Maximum  Name                                                                            \n",
      "-------  --------------  ----------  --------------  --------------  --------------  --------------------------------------------------------------------------------\n",
      "   82.5       201752431           3      67250810.3           27196       201660275  cudaMallocManaged                                                               \n",
      "    8.4        20525588           4       5131397.0           12849        19096111  cudaMemPrefetchAsync                                                            \n",
      "    7.8        19097082           3       6365694.0         5257283         8414430  cudaFree                                                                        \n",
      "    1.3         3222207           1       3222207.0         3222207         3222207  cudaDeviceSynchronize                                                           \n",
      "    0.0           71969           4         17992.3            8606           42043  cudaLaunchKernel                                                                \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Generating CUDA Kernel Statistics...\n",
      "\n",
      "Generating CUDA Memory Operation Statistics...\n",
      "CUDA Kernel Statistics (nanoseconds)\n",
      "\n",
      "Time(%)      Total Time   Instances         Average         Minimum         Maximum  Name                                                                            \n",
      "-------  --------------  ----------  --------------  --------------  --------------  --------------------------------------------------------------------------------\n",
      "   51.5          501843           1        501843.0          501843          501843  addVectorsInto                                                                  \n",
      "   48.5          472340           3        157446.7          155036          160860  initWith                                                                        \n",
      "\n",
      "\n",
      "CUDA Memory Operation Statistics (nanoseconds)\n",
      "\n",
      "Time(%)      Total Time  Operations         Average         Minimum         Maximum  Name                                                                            \n",
      "-------  --------------  ----------  --------------  --------------  --------------  --------------------------------------------------------------------------------\n",
      "  100.0        10651456          64        166429.0          166272          166880  [CUDA Unified Memory memcpy DtoH]                                               \n",
      "\n",
      "\n",
      "CUDA Memory Operation Statistics (KiB)\n",
      "\n",
      "            Total      Operations            Average            Minimum            Maximum  Name                                                                            \n",
      "-----------------  --------------  -----------------  -----------------  -----------------  --------------------------------------------------------------------------------\n",
      "         131072.0              64             2048.0           2048.000             2048.0  [CUDA Unified Memory memcpy DtoH]                                               \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Generating Operating System Runtime API Statistics...\n",
      "Operating System Runtime API Statistics (nanoseconds)\n",
      "\n",
      "Time(%)      Total Time       Calls         Average         Minimum         Maximum  Name                                                                            \n",
      "-------  --------------  ----------  --------------  --------------  --------------  --------------------------------------------------------------------------------\n",
      "   44.3       432595927          31      13954707.3            2378       100176935  poll                                                                            \n",
      "   43.3       422281206          26      16241584.8           23844       100126146  sem_timedwait                                                                   \n",
      "    9.9        96364672         579        166432.9            1032        19053917  ioctl                                                                           \n",
      "    2.2        21334667          88        242439.4            1519         8317735  mmap                                                                            \n",
      "    0.1          715817          96          7456.4            1759           25846  fopen                                                                           \n",
      "    0.1          669712           5        133942.4           33398          494963  pthread_create                                                                  \n",
      "    0.1          547956          78          7025.1            4147           16381  open64                                                                          \n",
      "    0.0          326797           3        108932.3           32645          199363  sem_wait                                                                        \n",
      "    0.0          260698          89          2929.2            1592            5487  fclose                                                                          \n",
      "    0.0          151547          14         10824.8            1605           19490  write                                                                           \n",
      "    0.0           95702           3         31900.7           25793           42044  fgets                                                                           \n",
      "    0.0           93901          81          1159.3            1002            6933  fcntl                                                                           \n",
      "    0.0           52904          12          4408.7            2128            7010  munmap                                                                          \n",
      "    0.0           40123           5          8024.6            4682           10435  open                                                                            \n",
      "    0.0           37633          16          2352.1            1363            4500  read                                                                            \n",
      "    0.0           17597           3          5865.7            5503            6410  pipe2                                                                           \n",
      "    0.0           14738           2          7369.0            4460           10278  socket                                                                          \n",
      "    0.0           14004           5          2800.8            2165            3813  mprotect                                                                        \n",
      "    0.0           11293           1         11293.0           11293           11293  connect                                                                         \n",
      "    0.0           11240           3          3746.7            2208            4690  fread                                                                           \n",
      "    0.0            7349           1          7349.0            7349            7349  pthread_cond_broadcast                                                          \n",
      "    0.0            2922           1          2922.0            2922            2922  bind                                                                            \n",
      "    0.0            2705           1          2705.0            2705            2705  listen                                                                          \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Generating NVTX Push-Pop Range Statistics...\n",
      "NVTX Push-Pop Range Statistics (nanoseconds)\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!nsys profile --stats=true -o prefetch-to-host-report ./prefetch-to-host"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Open this report file in Nsight Systems, and do the following:\n",
    "\n",
    "- Use the *Unified Memory* section of the timeline to compare and contrast the *Data Migration (DtoH)* events before and after adding prefetching back to the CPU."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Concurrent CUDA Streams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You are now going to learn about a new concept, **CUDA Streams**. After an introduction to them, you will return to using Nsight Systems to better evaluate their impact on your application's performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following slides present upcoming material visually, at a high level. Click through the slides before moving on to more detailed coverage of their topics in following sections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<div align=\"center\"><iframe src=\"https://docs.google.com/presentation/d/e/2PACX-1vRVgzpDzp5fWAu-Zpuyr09rmIqE4FTFESjajhfZSnY7yVvPgZUDxECAPSdLko5DZNTGEN7uA79Hfovd/embed?start=false&loop=false&delayms=3000\" frameborder=\"0\" width=\"900\" height=\"550\" allowfullscreen=\"true\" mozallowfullscreen=\"true\" webkitallowfullscreen=\"true\"></iframe></div>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%HTML\n",
    "\n",
    "<div align=\"center\"><iframe src=\"https://docs.google.com/presentation/d/e/2PACX-1vRVgzpDzp5fWAu-Zpuyr09rmIqE4FTFESjajhfZSnY7yVvPgZUDxECAPSdLko5DZNTGEN7uA79Hfovd/embed?start=false&loop=false&delayms=3000\" frameborder=\"0\" width=\"900\" height=\"550\" allowfullscreen=\"true\" mozallowfullscreen=\"true\" webkitallowfullscreen=\"true\"></iframe></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In CUDA programming, a **stream** is a series of commands that execute in order. In CUDA applications, kernel execution, as well as some memory transfers, occur within CUDA streams. Up until this point in time, you have not been interacting explicitly with CUDA streams, but in fact, your CUDA code has been executing its kernels inside of a stream called *the default stream*.\n",
    "\n",
    "CUDA programmers can create and utilize non-default CUDA streams in addition to the default stream, and in doing so, perform multiple operations, such as executing multiple kernels, concurrently, in different streams. Using multiple streams can add an additional layer of parallelization to your accelerated applications, and offers many more opportunities for application optimization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rules Governing the Behavior of CUDA Streams\n",
    "\n",
    "There are a few rules, concerning the behavior of CUDA streams, that should be learned in order to utilize them effectively:\n",
    "\n",
    "- Operations within a given stream occur in order.\n",
    "- Operations in different non-default streams are not guaranteed to operate in any specific order relative to each other.\n",
    "- The default stream is blocking and will both wait for all other streams to complete before running, and, will block other streams from running until it completes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating, Utilizing, and Destroying Non-Default CUDA Streams\n",
    "\n",
    "The following code snippet demonstrates how to create, utilize, and destroy a non-default CUDA stream. You will note, that to launch a CUDA kernel in a non-default CUDA stream, the stream must be passed as the optional 4th argument of the execution configuration. Up until now you have only utilized the first 2 arguments of the execution configuration:\n",
    "\n",
    "```cpp\n",
    "cudaStream_t stream;       // CUDA streams are of type `cudaStream_t`.\n",
    "cudaStreamCreate(&stream); // Note that a pointer must be passed to `cudaCreateStream`.\n",
    "\n",
    "someKernel<<<number_of_blocks, threads_per_block, 0, stream>>>(); // `stream` is passed as 4th EC argument.\n",
    "\n",
    "cudaStreamDestroy(stream); // Note that a value, not a pointer, is passed to `cudaDestroyStream`.\n",
    "```\n",
    "\n",
    "Outside the scope of this lab, but worth mentioning, is the optional 3rd argument of the execution configuration. This argument allows programmers to supply the number of bytes in **shared memory** (an advanced topic that will not be covered presently) to be dynamically allocated per block for this kernel launch. The default number of bytes allocated to shared memory per block is `0`, and for the remainder of the lab, you will be passing `0` as this value, in order to expose the 4th argument, which is of immediate interest:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Predict Default Stream Behavior\n",
    "\n",
    "The [01-print-numbers](../edit/05-stream-intro/01-print-numbers.cu) application has a very simple `printNumber` kernel which accepts an integer and prints it. The kernel is only being executed with a single thread inside a single block, however, it is being executed 5 times, using a for-loop, and passing each launch the number of the for-loop's iteration.\n",
    "\n",
    "Compile and run [01-print-numbers](../edit/05-stream-intro/01-print-numbers.cu) using the code execution block below. You should see the numbers `0` through `4` printed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\r\n",
      "1\r\n",
      "2\r\n",
      "3\r\n",
      "4\r\n"
     ]
    }
   ],
   "source": [
    "!nvcc -o print-numbers 05-stream-intro/01-print-numbers.cu -run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Knowing that by default kernels are executed in the default stream, would you expect that the 5 launches of the `print-numbers` program executed serially, or in parallel? You should be able to mention two features of the default stream to support your answer. Create a report file in the cell below and open it in Nsight Systems to confirm your answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "**** collection configuration ****\n",
      "\toutput_filename = /dli/task/print-numbers-report\n",
      "\tforce-overwrite = false\n",
      "\tstop-on-exit = true\n",
      "\texport_sqlite = true\n",
      "\tstats = true\n",
      "\tcapture-range = none\n",
      "\tstop-on-range-end = false\n",
      "\tBeta: ftrace events:\n",
      "\tftrace-keep-user-config = false\n",
      "\ttrace-GPU-context-switch = false\n",
      "\tdelay = 0 seconds\n",
      "\tduration = 0 seconds\n",
      "\tkill = signal number 15\n",
      "\tinherit-environment = true\n",
      "\tshow-output = true\n",
      "\ttrace-fork-before-exec = false\n",
      "\tsample_cpu = true\n",
      "\tbacktrace_method = LBR\n",
      "\twait = all\n",
      "\ttrace_cublas = false\n",
      "\ttrace_cuda = true\n",
      "\ttrace_cudnn = false\n",
      "\ttrace_nvtx = true\n",
      "\ttrace_mpi = false\n",
      "\ttrace_openacc = false\n",
      "\ttrace_vulkan = false\n",
      "\ttrace_opengl = true\n",
      "\ttrace_osrt = true\n",
      "\tosrt-threshold = 0 nanoseconds\n",
      "\tcudabacktrace = false\n",
      "\tcudabacktrace-threshold = 0 nanoseconds\n",
      "\tprofile_processes = tree\n",
      "\tapplication command = ./print-numbers\n",
      "\tapplication arguments = \n",
      "\tapplication working directory = /dli/task\n",
      "\tNVTX profiler range trigger = \n",
      "\tNVTX profiler domain trigger = \n",
      "\tenvironment variables:\n",
      "\tCollecting data...\n",
      "The application process terminated. One or more process it created re-parented. Waiting for termination of re-parented processes. To modify this behavior, use the `--wait` option.\n",
      "\u00000\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "\tGenerating the /dli/task/print-numbers-report.qdstrm file.\n",
      "\tCapturing raw events...\n",
      "\t1157 total events collected.\n",
      "\tSaving diagnostics...\n",
      "\tSaving qdstrm file to disk...\n",
      "\tFinished saving file.\n",
      "\n",
      "\n",
      "Importing the qdstrm file using /opt/nvidia/nsight-systems/2019.5.2/host-linux-x64/QdstrmImporter.\n",
      "\n",
      "Importing...\n",
      "\n",
      "Importing [==================================================100%]\n",
      "Saving report to file \"/dli/task/print-numbers-report.qdrep\"\n",
      "Report file saved.\n",
      "Please discard the qdstrm file and use the qdrep file instead.\n",
      "\n",
      "Removed /dli/task/print-numbers-report.qdstrm as it was successfully imported.\n",
      "Please use the qdrep file instead.\n",
      "\n",
      "Exporting the qdrep file to SQLite database using /opt/nvidia/nsight-systems/2019.5.2/host-linux-x64/nsys-exporter.\n",
      "\n",
      "Exporting 1121 events:\n",
      "\n",
      "0%   10   20   30   40   50   60   70   80   90   100%\n",
      "|----|----|----|----|----|----|----|----|----|----|\n",
      "***************************************************\n",
      "\n",
      "Exported successfully to\n",
      "/dli/task/print-numbers-report.sqlite\n",
      "\n",
      "Generating CUDA API Statistics...\n",
      "CUDA API Statistics (nanoseconds)\n",
      "\n",
      "Time(%)      Total Time       Calls         Average         Minimum         Maximum  Name                                                                            \n",
      "-------  --------------  ----------  --------------  --------------  --------------  --------------------------------------------------------------------------------\n",
      "   99.9       180744784           5      36148956.8            7125       180710540  cudaLaunchKernel                                                                \n",
      "    0.1          150788           1        150788.0          150788          150788  cudaDeviceSynchronize                                                           \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Generating CUDA Kernel Statistics...\n",
      "CUDA Kernel Statistics (nanoseconds)\n",
      "\n",
      "Time(%)      Total Time   Instances         Average         Minimum         Maximum  Name                                                                            \n",
      "-------  --------------  ----------  --------------  --------------  --------------  --------------------------------------------------------------------------------\n",
      "  100.0          174717           5         34943.4           32927           42528  printNumber                                                                     \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Generating Operating System Runtime API Statistics...\n",
      "Operating System Runtime API Statistics (nanoseconds)\n",
      "\n",
      "Time(%)      Total Time       Calls         Average         Minimum         Maximum  Name                                                                            \n",
      "-------  --------------  ----------  --------------  --------------  --------------  --------------------------------------------------------------------------------\n",
      "   44.8       213287651          13      16406742.4           24201       100120204  sem_timedwait                                                                   \n",
      "   37.6       179373589          12      14947799.1           37493        78506198  poll                                                                            \n",
      "   16.5        78759064         564        139643.7            1019        15508090  ioctl                                                                           \n",
      "    0.5         2617142          80         32714.3            1664          799654  mmap                                                                            \n",
      "    0.1          656005          97          6762.9            1770           17633  fopen                                                                           \n",
      "    0.1          629829           4        157457.2           37225          511286  pthread_create                                                                  \n",
      "    0.1          518428          79          6562.4            3802           12897  open64                                                                          \n",
      "    0.1          258386          90          2871.0            1621            4578  fclose                                                                          \n",
      "    0.0          132949          11         12086.3            6820           24028  write                                                                           \n",
      "    0.0           93862           3         31287.3           23982           42126  fgets                                                                           \n",
      "    0.0           83760          74          1131.9            1001            5617  fcntl                                                                           \n",
      "    0.0           34722           5          6944.4            4422            9704  open                                                                            \n",
      "    0.0           31687          13          2437.5            1341            3977  read                                                                            \n",
      "    0.0           30291           1         30291.0           30291           30291  sem_wait                                                                        \n",
      "    0.0           26624           7          3803.4            2586            6646  munmap                                                                          \n",
      "    0.0           16885           3          5628.3            4966            6908  pipe2                                                                           \n",
      "    0.0           12213           2          6106.5            4874            7339  socket                                                                          \n",
      "    0.0            9808           4          2452.0            2161            2662  mprotect                                                                        \n",
      "    0.0            8195           2          4097.5            3406            4789  fread                                                                           \n",
      "    0.0            8132           1          8132.0            8132            8132  connect                                                                         \n",
      "    0.0            5840           1          5840.0            5840            5840  pthread_cond_broadcast                                                          \n",
      "    0.0            5242           2          2621.0            1015            4227  fflush                                                                          \n",
      "    0.0            3015           1          3015.0            3015            3015  bind                                                                            \n",
      "    0.0            2182           1          2182.0            2182            2182  listen                                                                          \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Generating NVTX Push-Pop Range Statistics...\n",
      "NVTX Push-Pop Range Statistics (nanoseconds)\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!nsys profile --stats=true -o print-numbers-report ./print-numbers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Implement Concurrent CUDA Streams\n",
    "\n",
    "Both because all 5 kernel launches occured in the same stream, you should not be surprised to have seen that the 5 kernels executed serially. Additionally you could make the case that because the default stream is blocking, each launch of the kernel would wait to complete before the next launch, and this is also true.\n",
    "\n",
    "Refactor [01-print-numbers](../edit/05-stream-intro/01-print-numbers.cu) so that each kernel launch occurs in its own non-default stream. Be sure to destroy the streams you create after they are no longer needed. Compile and run the refactored code with the code execution cell directly below. You should still see the numbers `0` through `4` printed, though not necessarily in ascending order. Refer to [the solution](../edit/05-stream-intro/solutions/01-print-numbers-solution.cu) if you get stuck."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\r\n",
      "1\r\n",
      "2\r\n",
      "3\r\n",
      "4\r\n"
     ]
    }
   ],
   "source": [
    "!nvcc -o print-numbers-in-streams 05-stream-intro/01-print-numbers.cu -run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you are using 5 different non-default streams for each of the 5 kernel launches, do you expect that they will run serially or in parallel? In addition to what you now know about streams, take into account how trivial the `printNumber` kernel is, meaning, even if you predict parallel runs, will the speed at which one kernel will complete allow for complete overlap?\n",
    "\n",
    "After hypothesizing, open a new report file in Nsight Systems to view its actual behavior. You should notice that now, there are additional rows in the _CUDA_ section for each of the non-default streams you created:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "**** collection configuration ****\n",
      "\toutput_filename = /dli/task/print-numbers-in-streams-report\n",
      "\tforce-overwrite = false\n",
      "\tstop-on-exit = true\n",
      "\texport_sqlite = true\n",
      "\tstats = true\n",
      "\tcapture-range = none\n",
      "\tstop-on-range-end = false\n",
      "\tBeta: ftrace events:\n",
      "\tftrace-keep-user-config = false\n",
      "\ttrace-GPU-context-switch = false\n",
      "\tdelay = 0 seconds\n",
      "\tduration = 0 seconds\n",
      "\tkill = signal number 15\n",
      "\tinherit-environment = true\n",
      "\tshow-output = true\n",
      "\ttrace-fork-before-exec = false\n",
      "\tsample_cpu = true\n",
      "\tbacktrace_method = LBR\n",
      "\twait = all\n",
      "\ttrace_cublas = false\n",
      "\ttrace_cuda = true\n",
      "\ttrace_cudnn = false\n",
      "\ttrace_nvtx = true\n",
      "\ttrace_mpi = false\n",
      "\ttrace_openacc = false\n",
      "\ttrace_vulkan = false\n",
      "\ttrace_opengl = true\n",
      "\ttrace_osrt = true\n",
      "\tosrt-threshold = 0 nanoseconds\n",
      "\tcudabacktrace = false\n",
      "\tcudabacktrace-threshold = 0 nanoseconds\n",
      "\tprofile_processes = tree\n",
      "\tapplication command = print-numbers-in-streams\n",
      "\tapplication arguments = \n",
      "\tapplication working directory = /dli/task\n",
      "\tNVTX profiler range trigger = \n",
      "\tNVTX profiler domain trigger = \n",
      "\tenvironment variables:\n",
      "\tCollecting data...\n",
      "The application process terminated. One or more process it created re-parented. Waiting for termination of re-parented processes. To modify this behavior, use the `--wait` option.\n",
      "\u00000\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "\tGenerating the /dli/task/print-numbers-in-streams-report.qdstrm file.\n",
      "\tCapturing raw events...\n",
      "\t1189 total events collected.\n",
      "\tSaving diagnostics...\n",
      "\tSaving qdstrm file to disk...\n",
      "\tFinished saving file.\n",
      "\n",
      "\n",
      "Importing the qdstrm file using /opt/nvidia/nsight-systems/2019.5.2/host-linux-x64/QdstrmImporter.\n",
      "\n",
      "Importing...\n",
      "\n",
      "Importing [==================================================100%]\n",
      "Saving report to file \"/dli/task/print-numbers-in-streams-report.qdrep\"\n",
      "Report file saved.\n",
      "Please discard the qdstrm file and use the qdrep file instead.\n",
      "\n",
      "Removed /dli/task/print-numbers-in-streams-report.qdstrm as it was successfully imported.\n",
      "Please use the qdrep file instead.\n",
      "\n",
      "Exporting the qdrep file to SQLite database using /opt/nvidia/nsight-systems/2019.5.2/host-linux-x64/nsys-exporter.\n",
      "\n",
      "Exporting 1148 events:\n",
      "\n",
      "0%   10   20   30   40   50   60   70   80   90   100%\n",
      "|----|----|----|----|----|----|----|----|----|----|\n",
      "***************************************************\n",
      "\n",
      "Exported successfully to\n",
      "/dli/task/print-numbers-in-streams-report.sqlite\n",
      "\n",
      "Generating CUDA API Statistics...\n",
      "CUDA API Statistics (nanoseconds)\n",
      "\n",
      "Time(%)      Total Time       Calls         Average         Minimum         Maximum  Name                                                                            \n",
      "-------  --------------  ----------  --------------  --------------  --------------  --------------------------------------------------------------------------------\n",
      "   99.9       188651597           5      37730319.4            4191       188633329  cudaStreamCreate                                                                \n",
      "    0.1          115596           5         23119.2           10659           68420  cudaLaunchKernel                                                                \n",
      "    0.0           47741           1         47741.0           47741           47741  cudaDeviceSynchronize                                                           \n",
      "    0.0           45979           5          9195.8            5647           18516  cudaStreamDestroy                                                               \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Generating CUDA Kernel Statistics...\n",
      "CUDA Kernel Statistics (nanoseconds)\n",
      "\n",
      "Time(%)      Total Time   Instances         Average         Minimum         Maximum  Name                                                                            \n",
      "-------  --------------  ----------  --------------  --------------  --------------  --------------------------------------------------------------------------------\n",
      "  100.0          192093           5         38418.6           34911           46367  printNumber                                                                     \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Generating Operating System Runtime API Statistics...\n",
      "Operating System Runtime API Statistics (nanoseconds)\n",
      "\n",
      "Time(%)      Total Time       Calls         Average         Minimum         Maximum  Name                                                                            \n",
      "-------  --------------  ----------  --------------  --------------  --------------  --------------------------------------------------------------------------------\n",
      "   44.6       222219290          13      17093791.5           24156       100136314  sem_timedwait                                                                   \n",
      "   37.6       187297824          12      15608152.0           38619        86469225  poll                                                                            \n",
      "   16.7        83297696         573        145371.2            1061        15951841  ioctl                                                                           \n",
      "    0.5         2704242          80         33803.0            1746          781312  mmap                                                                            \n",
      "    0.2          827854          97          8534.6            1885           23090  fopen                                                                           \n",
      "    0.1          636353           4        159088.2           46977          488057  pthread_create                                                                  \n",
      "    0.1          609192          79          7711.3            4208           14803  open64                                                                          \n",
      "    0.1          282424          90          3138.0            1567            5552  fclose                                                                          \n",
      "    0.0          132988          11         12089.8            7179           16884  write                                                                           \n",
      "    0.0           98685           3         32895.0           26797           42758  fgets                                                                           \n",
      "    0.0           97096          82          1184.1            1017            7044  fcntl                                                                           \n",
      "    0.0           44179           5          8835.8            4510           11544  open                                                                            \n",
      "    0.0           31164          13          2397.2            1317            3958  read                                                                            \n",
      "    0.0           30603           7          4371.9            2688            7328  munmap                                                                          \n",
      "    0.0           28542           1         28542.0           28542           28542  sem_wait                                                                        \n",
      "    0.0           19110           2          9555.0            7261           11849  socket                                                                          \n",
      "    0.0           17805           3          5935.0            5458            6663  pipe2                                                                           \n",
      "    0.0           11868           1         11868.0           11868           11868  connect                                                                         \n",
      "    0.0           10854           4          2713.5            2377            3397  mprotect                                                                        \n",
      "    0.0            9538           2          4769.0            4609            4929  fread                                                                           \n",
      "    0.0            6175           1          6175.0            6175            6175  pthread_cond_broadcast                                                          \n",
      "    0.0            6038           2          3019.0            1670            4368  fflush                                                                          \n",
      "    0.0            3837           1          3837.0            3837            3837  bind                                                                            \n",
      "    0.0            2469           1          2469.0            2469            2469  listen                                                                          \n",
      "\n",
      "\n",
      "\n",
      "\r\n",
      "Generating NVTX Push-Pop Range Statistics...\r\n",
      "NVTX Push-Pop Range Statistics (nanoseconds)\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "!nsys profile --stats=true -o print-numbers-in-streams-report print-numbers-in-streams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![streams overlap](images/streams-overlap.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Use Streams for Concurrent Data Initialization Kernels\n",
    "\n",
    "The vector addition application you have been working with, [01-prefetch-check-solution.cu](../edit/04-prefetch-check/solutions/01-prefetch-check-solution.cu), currently launches an initialization kernel 3 times - once each for each of the 3 vectors needing initialization for the `vectorAdd` kernel. Refactor it to launch each of the 3 initialization kernel launches in their own non-default stream. You should still be see the success message print when compiling and running with the code execution cell below. Refer to [the solution](../edit/06-stream-init/solutions/01-stream-init-solution.cu) if you get stuck."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success! All values calculated correctly.\r\n"
     ]
    }
   ],
   "source": [
    "!nvcc -o init-in-streams 04-prefetch-check/solutions/01-prefetch-check-solution.cu -run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Open a report in Nsight Systems to confirm that your 3 initialization kernel launches are running in their own non-default streams, with some degree of concurrent overlap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "**** collection configuration ****\n",
      "\toutput_filename = /dli/task/init-in-streams-report\n",
      "\tforce-overwrite = false\n",
      "\tstop-on-exit = true\n",
      "\texport_sqlite = true\n",
      "\tstats = true\n",
      "\tcapture-range = none\n",
      "\tstop-on-range-end = false\n",
      "\tBeta: ftrace events:\n",
      "\tftrace-keep-user-config = false\n",
      "\ttrace-GPU-context-switch = false\n",
      "\tdelay = 0 seconds\n",
      "\tduration = 0 seconds\n",
      "\tkill = signal number 15\n",
      "\tinherit-environment = true\n",
      "\tshow-output = true\n",
      "\ttrace-fork-before-exec = false\n",
      "\tsample_cpu = true\n",
      "\tbacktrace_method = LBR\n",
      "\twait = all\n",
      "\ttrace_cublas = false\n",
      "\ttrace_cuda = true\n",
      "\ttrace_cudnn = false\n",
      "\ttrace_nvtx = true\n",
      "\ttrace_mpi = false\n",
      "\ttrace_openacc = false\n",
      "\ttrace_vulkan = false\n",
      "\ttrace_opengl = true\n",
      "\ttrace_osrt = true\n",
      "\tosrt-threshold = 0 nanoseconds\n",
      "\tcudabacktrace = false\n",
      "\tcudabacktrace-threshold = 0 nanoseconds\n",
      "\tprofile_processes = tree\n",
      "\tapplication command = ./init-in-streams\n",
      "\tapplication arguments = \n",
      "\tapplication working directory = /dli/task\n",
      "\tNVTX profiler range trigger = \n",
      "\tNVTX profiler domain trigger = \n",
      "\tenvironment variables:\n",
      "\tCollecting data...\n",
      "Success! All values calculated correctly.\n",
      "\tGenerating the /dli/task/init-in-streams-report.qdstrm file.\n",
      "\tCapturing raw events...\n",
      "\t1317 total events collected.\n",
      "\tSaving diagnostics...\n",
      "\tSaving qdstrm file to disk...\n",
      "\tFinished saving file.\n",
      "\n",
      "\n",
      "Importing the qdstrm file using /opt/nvidia/nsight-systems/2019.5.2/host-linux-x64/QdstrmImporter.\n",
      "\n",
      "Importing...\n",
      "\n",
      "Importing [==================================================100%]\n",
      "Saving report to file \"/dli/task/init-in-streams-report.qdrep\"\n",
      "Report file saved.\n",
      "Please discard the qdstrm file and use the qdrep file instead.\n",
      "\n",
      "Removed /dli/task/init-in-streams-report.qdstrm as it was successfully imported.\n",
      "Please use the qdrep file instead.\n",
      "\n",
      "Exporting the qdrep file to SQLite database using /opt/nvidia/nsight-systems/2019.5.2/host-linux-x64/nsys-exporter.\n",
      "\n",
      "Exporting 1277 events:\n",
      "\n",
      "0%   10   20   30   40   50   60   70   80   90   100%\n",
      "|----|----|----|----|----|----|----|----|----|----|\n",
      "***************************************************\n",
      "\n",
      "Exported successfully to\n",
      "/dli/task/init-in-streams-report.sqlite\n",
      "\n",
      "Generating CUDA API Statistics...\n",
      "CUDA API Statistics (nanoseconds)\n",
      "\n",
      "Time(%)      Total Time       Calls         Average         Minimum         Maximum  Name                                                                            \n",
      "-------  --------------  ----------  --------------  --------------  --------------  --------------------------------------------------------------------------------\n",
      "   83.0       206024529           3      68674843.0           26641       205908100  cudaMallocManaged                                                               \n",
      "    8.1        20237798           4       5059449.5           14249        19191943  cudaMemPrefetchAsync                                                            \n",
      "    7.5        18600548           3       6200182.7         5176296         8152255  cudaFree                                                                        \n",
      "    1.3         3323030           1       3323030.0         3323030         3323030  cudaDeviceSynchronize                                                           \n",
      "    0.0           84398           4         21099.5           11166           47935  cudaLaunchKernel                                                                \n",
      "    0.0           59169           3         19723.0            5936           47251  cudaStreamDestroy                                                               \n",
      "    0.0           24438           3          8146.0            3667           16916  cudaStreamCreate                                                                \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Generating CUDA Kernel Statistics...\n",
      "\n",
      "Generating CUDA Memory Operation Statistics...\n",
      "CUDA Kernel Statistics (nanoseconds)\n",
      "\n",
      "Time(%)      Total Time   Instances         Average         Minimum         Maximum  Name                                                                            \n",
      "-------  --------------  ----------  --------------  --------------  --------------  --------------------------------------------------------------------------------\n",
      "   55.0          610962           3        203654.0          186460          223611  initWith                                                                        \n",
      "   45.0          499284           1        499284.0          499284          499284  addVectorsInto                                                                  \n",
      "\n",
      "\n",
      "CUDA Memory Operation Statistics (nanoseconds)\n",
      "\n",
      "Time(%)      Total Time  Operations         Average         Minimum         Maximum  Name                                                                            \n",
      "-------  --------------  ----------  --------------  --------------  --------------  --------------------------------------------------------------------------------\n",
      "  100.0        12368160          64        193252.5          187264          197696  [CUDA Unified Memory memcpy DtoH]                                               \n",
      "\n",
      "\n",
      "CUDA Memory Operation Statistics (KiB)\n",
      "\n",
      "            Total      Operations            Average            Minimum            Maximum  Name                                                                            \n",
      "-----------------  --------------  -----------------  -----------------  -----------------  --------------------------------------------------------------------------------\n",
      "         131072.0              64             2048.0           2048.000             2048.0  [CUDA Unified Memory memcpy DtoH]                                               \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Generating Operating System Runtime API Statistics...\n",
      "Operating System Runtime API Statistics (nanoseconds)\n",
      "\n",
      "Time(%)      Total Time       Calls         Average         Minimum         Maximum  Name                                                                            \n",
      "-------  --------------  ----------  --------------  --------------  --------------  --------------------------------------------------------------------------------\n",
      "   44.0       432907687          31      13964764.1            2843       100184642  poll                                                                            \n",
      "   43.3       426430913          26      16401189.0           21825       100122366  sem_timedwait                                                                   \n",
      "   10.1        99656161         583        170936.8            1067        19139598  ioctl                                                                           \n",
      "    2.2        21181460          88        240698.4            1595         8098577  mmap                                                                            \n",
      "    0.1          894889          96          9321.8            1792           26341  fopen                                                                           \n",
      "    0.1          680227           5        136045.4           37311          486384  pthread_create                                                                  \n",
      "    0.1          626671          78          8034.2            4509           17320  open64                                                                          \n",
      "    0.0          456844           4        114211.0           32885          273331  sem_wait                                                                        \n",
      "    0.0          296653          89          3333.2            1552            6419  fclose                                                                          \n",
      "    0.0          160499          14         11464.2            1883           22589  write                                                                           \n",
      "    0.0          105089          82          1281.6            1000            6760  fcntl                                                                           \n",
      "    0.0           94719           3         31573.0           25002           41811  fgets                                                                           \n",
      "    0.0           50981          11          4634.6            2203            8152  munmap                                                                          \n",
      "    0.0           43076           5          8615.2            4548           12087  open                                                                            \n",
      "    0.0           38657          16          2416.1            1285            4335  read                                                                            \n",
      "    0.0           21160           3          7053.3            4984            9528  pipe2                                                                           \n",
      "    0.0           18360           2          9180.0            6240           12120  socket                                                                          \n",
      "    0.0           13144           5          2628.8            2029            3319  mprotect                                                                        \n",
      "    0.0           11991           1         11991.0           11991           11991  connect                                                                         \n",
      "    0.0           11789           3          3929.7            2305            4968  fread                                                                           \n",
      "    0.0            6977           1          6977.0            6977            6977  pthread_cond_broadcast                                                          \n",
      "    0.0            3149           1          3149.0            3149            3149  bind                                                                            \n",
      "    0.0            2160           1          2160.0            2160            2160  listen                                                                          \n",
      "\n",
      "\n",
      "\n",
      "\r\n",
      "Generating NVTX Push-Pop Range Statistics...\r\n",
      "NVTX Push-Pop Range Statistics (nanoseconds)\r\n",
      "\r\n",
      "\r\n",
      "\r\n",
      "\r\n"
     ]
    }
   ],
   "source": [
    "!nsys profile --stats=true -o init-in-streams-report ./init-in-streams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary\n",
    "\n",
    "At this point in the lab you are able to:\n",
    "\n",
    "- Use the **Nsight Systems** to visually profile the timeline of GPU-accelerated CUDA applications.\n",
    "- Use Nsight Systems to identify, and exploit, optimization opportunities in GPU-accelerated CUDA applications.\n",
    "- Utilize CUDA streams for concurrent kernel execution in accelerated applications.\n",
    "\n",
    "At this point in time you have a wealth of fundamental tools and techniques for accelerating CPU-only applications, and for then optimizing those accelerated applications. In the final exercise, you will have a chance to apply everything that you've learned to accelerate an [n-body](https://en.wikipedia.org/wiki/N-body_problem) simulator, which predicts the individual motions of a group of objects interacting with each other gravitationally."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Final Exercise: Accelerate and Optimize an N-Body Simulator\n",
    "\n",
    "An [n-body](https://en.wikipedia.org/wiki/N-body_problem) simulator predicts the individual motions of a group of objects interacting with each other gravitationally. [01-nbody.cu](../edit/09-nbody/01-nbody.cu) contains a simple, though working, n-body simulator for bodies moving through 3 dimensional space. The application can be passed a command line argument to affect how many bodies are in the system.\n",
    "\n",
    "In its current CPU-only form, working on 4096 bodies, this application is able to calculate about 30 million interactions between bodies in the system per second. Your task is to:\n",
    "\n",
    "- GPU accelerate the program, retaining the correctness of the simulation\n",
    "- Work iteratively to optimize the simulator so that it calculates over 30 billion interactions per second while working on 4096 bodies `(2<<11)`\n",
    "- Work iteratively to optimize the simulator so that it calculates over 325 billion interactions per second while working on ~65,000 bodies `(2<<15)`\n",
    "\n",
    "**After completing this, go back in the browser page you used to open this notebook and click the Assess button. If you have retained the accuracy of the application and accelerated it to the specifications above, you will receive a certification for your competency in the _Fundamentals of Accelerating Applications with CUDA C/C++_.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Considerations to Guide Your Work\n",
    "\n",
    "Here are some things to consider before beginning your work:\n",
    "\n",
    "- Especially for your first refactors, the logic of the application, the `bodyForce` function in particular, can and should remain largely unchanged: focus on accelerating it as easily as possible.\n",
    "- You will not be able to accelerate the `randomizeBodies` function since it is using the `rand` function, which is not available on GPU devices. `randomizeBodies` is a host function. Do not touch it at all.\n",
    "- The codebase contains a for-loop inside `main` for integrating the interbody forces calculated by `bodyForce` into the positions of the bodies in the system. This integration both needs to occur after `bodyForce` runs, and, needs to complete before the next call to `bodyForce`. Keep this in mind when choosing how and where to parallelize.\n",
    "- Use a **profile driven** and iterative approach.\n",
    "- You are not required to add error handling to your code, but you might find it helpful, as you are responsible for your code working correctly.\n",
    "\n",
    "Have Fun!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvcc -o nbody 09-nbody/01-nbody.cu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simulator is calculating positions correctly.\r\n",
      "4096 Bodies: average 45.136 Billion Interactions / second\r\n"
     ]
    }
   ],
   "source": [
    "!./nbody 11 # This argument is passed as `N` in the formula `2<<N`, to determine the number of bodies in the system"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Don't forget that you can use the `-f` flag to force the overwrite of an existing report file, so that you do not need to keep multiple report files around during development."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "**** collection configuration ****\n",
      "\toutput_filename = /dli/task/nbody-report\n",
      "\tforce-overwrite = false\n",
      "\tstop-on-exit = true\n",
      "\texport_sqlite = true\n",
      "\tstats = true\n",
      "\tcapture-range = none\n",
      "\tstop-on-range-end = false\n",
      "\tBeta: ftrace events:\n",
      "\tftrace-keep-user-config = false\n",
      "\ttrace-GPU-context-switch = false\n",
      "\tdelay = 0 seconds\n",
      "\tduration = 0 seconds\n",
      "\tkill = signal number 15\n",
      "\tinherit-environment = true\n",
      "\tshow-output = true\n",
      "\ttrace-fork-before-exec = false\n",
      "\tsample_cpu = true\n",
      "\tbacktrace_method = LBR\n",
      "\twait = all\n",
      "\ttrace_cublas = false\n",
      "\ttrace_cuda = true\n",
      "\ttrace_cudnn = false\n",
      "\ttrace_nvtx = true\n",
      "\ttrace_mpi = false\n",
      "\ttrace_openacc = false\n",
      "\ttrace_vulkan = false\n",
      "\ttrace_opengl = true\n",
      "\ttrace_osrt = true\n",
      "\tosrt-threshold = 0 nanoseconds\n",
      "\tcudabacktrace = false\n",
      "\tcudabacktrace-threshold = 0 nanoseconds\n",
      "\tprofile_processes = tree\n",
      "\tapplication command = ./nbody\n",
      "\tapplication arguments = \n",
      "\tapplication working directory = /dli/task\n",
      "\tNVTX profiler range trigger = \n",
      "\tNVTX profiler domain trigger = \n",
      "\tenvironment variables:\n",
      "\tCollecting data...\n",
      "Simulator is calculating positions correctly.\n",
      "4096 Bodies: average 43.408 Billion Interactions / second\n",
      "\tGenerating the /dli/task/nbody-report.qdstrm file.\n",
      "\tCapturing raw events...\n",
      "\t1247 total events collected.\n",
      "\tSaving diagnostics...\n",
      "\tSaving qdstrm file to disk...\n",
      "\tFinished saving file.\n",
      "\n",
      "\n",
      "Importing the qdstrm file using /opt/nvidia/nsight-systems/2019.5.2/host-linux-x64/QdstrmImporter.\n",
      "\n",
      "Importing...\n",
      "\n",
      "Importing [==================================================100%]\n",
      "Saving report to file \"/dli/task/nbody-report.qdrep\"\n",
      "Report file saved.\n",
      "Please discard the qdstrm file and use the qdrep file instead.\n",
      "\n",
      "Removed /dli/task/nbody-report.qdstrm as it was successfully imported.\n",
      "Please use the qdrep file instead.\n",
      "\n",
      "Exporting the qdrep file to SQLite database using /opt/nvidia/nsight-systems/2019.5.2/host-linux-x64/nsys-exporter.\n",
      "\n",
      "Exporting 1209 events:\n",
      "\n",
      "0%   10   20   30   40   50   60   70   80   90   100%\n",
      "|----|----|----|----|----|----|----|----|----|----|\n",
      "***************************************************\n",
      "\n",
      "Exported successfully to\n",
      "/dli/task/nbody-report.sqlite\n",
      "\n",
      "Generating CUDA API Statistics...\n",
      "CUDA API Statistics (nanoseconds)\n",
      "\n",
      "Time(%)      Total Time       Calls         Average         Minimum         Maximum  Name                                                                            \n",
      "-------  --------------  ----------  --------------  --------------  --------------  --------------------------------------------------------------------------------\n",
      "   97.9       204297055           1     204297055.0       204297055       204297055  cudaMallocManaged                                                               \n",
      "    1.7         3611472          20        180573.6            7928          354159  cudaDeviceSynchronize                                                           \n",
      "    0.2          357448           2        178724.0           23176          334272  cudaMemPrefetchAsync                                                            \n",
      "    0.1          220132           1        220132.0          220132          220132  cudaFree                                                                        \n",
      "    0.1          208522          20         10426.1            7621           36426  cudaLaunchKernel                                                                \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Generating CUDA Kernel Statistics...\n",
      "\n",
      "Generating CUDA Memory Operation Statistics...\n",
      "CUDA Kernel Statistics (nanoseconds)\n",
      "\n",
      "Time(%)      Total Time   Instances         Average         Minimum         Maximum  Name                                                                            \n",
      "-------  --------------  ----------  --------------  --------------  --------------  --------------------------------------------------------------------------------\n",
      "   98.5         3486352          10        348635.2          348280          351256  bodyForce                                                                       \n",
      "    1.5           52220          10          5222.0            5151            5632  Pos_Integration                                                                 \n",
      "\n",
      "\n",
      "CUDA Memory Operation Statistics (nanoseconds)\n",
      "\n",
      "Time(%)      Total Time  Operations         Average         Minimum         Maximum  Name                                                                            \n",
      "-------  --------------  ----------  --------------  --------------  --------------  --------------------------------------------------------------------------------\n",
      "   60.5           15136           1         15136.0           15136           15136  [CUDA Unified Memory memcpy HtoD]                                               \n",
      "   39.5            9888           2          4944.0            1824            8064  [CUDA Unified Memory memcpy DtoH]                                               \n",
      "\n",
      "\n",
      "CUDA Memory Operation Statistics (KiB)\n",
      "\n",
      "            Total      Operations            Average            Minimum            Maximum  Name                                                                            \n",
      "-----------------  --------------  -----------------  -----------------  -----------------  --------------------------------------------------------------------------------\n",
      "             96.0               1               96.0             96.000               96.0  [CUDA Unified Memory memcpy HtoD]                                               \n",
      "             96.0               2               48.0              4.000               92.0  [CUDA Unified Memory memcpy DtoH]                                               \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Generating Operating System Runtime API Statistics...\n",
      "Operating System Runtime API Statistics (nanoseconds)\n",
      "\n",
      "Time(%)      Total Time       Calls         Average         Minimum         Maximum  Name                                                                            \n",
      "-------  --------------  ----------  --------------  --------------  --------------  --------------------------------------------------------------------------------\n",
      "   48.1       298937297          15      19929153.1           21261       100148677  sem_timedwait                                                                   \n",
      "   38.7       240389527          16      15024345.4           36860       100180003  poll                                                                            \n",
      "   12.3        76548148         566        135244.1            1025        16271648  ioctl                                                                           \n",
      "    0.5         2820703          81         34823.5            1794          778635  mmap                                                                            \n",
      "    0.1          737150          96          7678.6            1762           28495  fopen                                                                           \n",
      "    0.1          536670          78          6880.4            5009           20738  open64                                                                          \n",
      "    0.0          275911          89          3100.1            1604            5771  fclose                                                                          \n",
      "    0.0          186323           4         46580.8           32306           58267  pthread_create                                                                  \n",
      "    0.0          123948          10         12394.8            6687           18610  write                                                                           \n",
      "    0.0           98925           3         32975.0           26675           42072  fgets                                                                           \n",
      "    0.0           95369          82          1163.0            1000            5660  fcntl                                                                           \n",
      "    0.0           41015           5          8203.0            5315           10219  open                                                                            \n",
      "    0.0           34328           8          4291.0            2420            7280  munmap                                                                          \n",
      "    0.0           27981          12          2331.8            1323            3849  read                                                                            \n",
      "    0.0           19724           2          9862.0            7797           11927  socket                                                                          \n",
      "    0.0           19444           1         19444.0           19444           19444  pthread_mutex_trylock                                                           \n",
      "    0.0           18531           3          6177.0            5441            7210  pipe2                                                                           \n",
      "    0.0           14367           4          3591.8            2960            5050  fread                                                                           \n",
      "    0.0           12106           1         12106.0           12106           12106  connect                                                                         \n",
      "    0.0           11454           4          2863.5            2088            3203  mprotect                                                                        \n",
      "    0.0            7009           1          7009.0            7009            7009  pthread_cond_broadcast                                                          \n",
      "    0.0            6113           1          6113.0            6113            6113  bind                                                                            \n",
      "    0.0            2335           1          2335.0            2335            2335  listen                                                                          \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Generating NVTX Push-Pop Range Statistics...\n",
      "NVTX Push-Pop Range Statistics (nanoseconds)\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!nsys profile --stats=true -o nbody-report ./nbody"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Advanced Content\n",
    "\n",
    "The following sections, for those of you with time and interest, introduce more intermediate techniques involving some manual device memory management, and using non-default streams to overlap kernel execution and memory copies.\n",
    "\n",
    "After learning about each of the techniques below, try to further optimize your nbody simulation using these techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Manual Device Memory Allocation and Copying\n",
    "\n",
    "While `cudaMallocManaged` and `cudaMemPrefetchAsync` are performant, and greatly simplify memory migration, sometimes it can be worth it to use more manual methods for memory allocation. This is particularly true when it is known that data will only be accessed on the device or host, and the cost of migrating data can be reclaimed in exchange for the fact that no automatic on-demand migration is needed.\n",
    "\n",
    "Additionally, using manual device memory management can allow for the use of non-default streams for overlapping data transfers with computational work. In this section you will learn some basic manual device memory allocation and copy techniques, before extending these techniques to overlap data copies with computational work. \n",
    "\n",
    "Here are some CUDA commands for manual device memory management:\n",
    "\n",
    "- `cudaMalloc` will allocate memory directly to the active GPU. This prevents all GPU page faults. In exchange, the pointer it returns is not available for access by host code.\n",
    "- `cudaMallocHost` will allocate memory directly to the CPU. It also \"pins\" the memory, or page locks it, which will allow for asynchronous copying of the memory to and from a GPU. Too much pinned memory can interfere with CPU performance, so use it only with intention. Pinned memory should be freed with `cudaFreeHost`.\n",
    "- `cudaMemcpy` can copy (not transfer) memory, either from host to device or from device to host."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manual Device Memory Management Example\n",
    "\n",
    "Here is a snippet of code that demonstrates the use of the above CUDA API calls.\n",
    "\n",
    "```cpp\n",
    "int *host_a, *device_a;        // Define host-specific and device-specific arrays.\n",
    "cudaMalloc(&device_a, size);   // `device_a` is immediately available on the GPU.\n",
    "cudaMallocHost(&host_a, size); // `host_a` is immediately available on CPU, and is page-locked, or pinned.\n",
    "\n",
    "initializeOnHost(host_a, N);   // No CPU page faulting since memory is already allocated on the host.\n",
    "\n",
    "// `cudaMemcpy` takes the destination, source, size, and a CUDA-provided variable for the direction of the copy.\n",
    "cudaMemcpy(device_a, host_a, size, cudaMemcpyHostToDevice);\n",
    "\n",
    "kernel<<<blocks, threads, 0, someStream>>>(device_a, N);\n",
    "\n",
    "// `cudaMemcpy` can also copy data from device to host.\n",
    "cudaMemcpy(host_a, device_a, size, cudaMemcpyDeviceToHost);\n",
    "\n",
    "verifyOnHost(host_a, N);\n",
    "\n",
    "cudaFree(device_a);\n",
    "cudaFreeHost(host_a);          // Free pinned memory like this.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Manually Allocate Host and Device Memory\n",
    "\n",
    "The most recent iteration of the vector addition application, [01-stream-init-solution](../edit/06-stream-init/solutions/01-stream-init-solution.cu), is using `cudaMallocManaged` to allocate managed memory first used on the device by the initialization kernels, then on the device by the vector add kernel, and then by the host, where the memory is automatically transfered, for verification. This is a sensible approach, but it is worth experimenting with some manual device memory allocation and copying to observe its impact on the application's performance.\n",
    "\n",
    "Refactor the [01-stream-init-solution](../edit/06-stream-init/solutions/01-stream-init-solution.cu) application to **not** use `cudaMallocManaged`. In order to do this you will need to do the following:\n",
    "\n",
    "- Replace calls to `cudaMallocManaged` with `cudaMalloc`.\n",
    "- Create an additional vector that will be used for verification on the host. This is required since the memory allocated with `cudaMalloc` is not available to the host. Allocate this host vector with `cudaMallocHost`.\n",
    "- After the `addVectorsInto` kernel completes, use `cudaMemcpy` to copy the vector with the addition results, into the host vector you created with `cudaMallocHost`.\n",
    "- Use `cudaFreeHost` to free the memory allocated with `cudaMallocHost`.\n",
    "\n",
    "Refer to [the solution](../edit/07-manual-malloc/solutions/01-manual-malloc-solution.cu) if you get stuck."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvcc -o vector-add-manual-alloc 06-stream-init/solutions/01-stream-init-solution.cu -run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After completing the refactor, open a report in Nsight Systems, and use the timeline to do the following:\n",
    "\n",
    "- Notice that there is no longer a *Unified Memory* section of the timeline.\n",
    "- Comparing this timeline to that of the previous refactor, compare the runtimes of `cudaMalloc` in the current application vs. `cudaMallocManaged` in the previous.\n",
    "- Notice how in the current application, work on the initialization kernels does not start until a later time than it did in the previous iteration. Examination of the timeline will show the difference is the time taken by `cudaMallocHost`. This clearly points out the difference between memory transfers, and memory copies. When copying memory, as you are doing presently, the data will exist in 2 different places in the system. In the current case, the allocation of the 4th host-only vector incurs a small cost in performance, compared to only allocating 3 vectors in the previous iteration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Using Streams to Overlap Data Transfers and Code Execution\n",
    "\n",
    "The following slides present upcoming material visually, at a high level. Click through the slides before moving on to more detailed coverage of their topics in following sections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%HTML\n",
    "\n",
    "<div align=\"center\"><iframe src=\"https://docs.google.com/presentation/d/e/2PACX-1vQdHDR62S4hhvq02CZreC_Hvb9y89_IRIKtCQQ-eMItim744eRHOK6Gead5P_EaPj66Z3_NS0hlTRuh/embed?start=false&loop=false&delayms=3000\" frameborder=\"0\" width=\"900\" height=\"550\" allowfullscreen=\"true\" mozallowfullscreen=\"true\" webkitallowfullscreen=\"true\"></iframe></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to `cudaMemcpy` is `cudaMemcpyAsync` which can asynchronously copy memory either from host to device or from device to host as long as the host memory is pinned, which can be done by allocating it with `cudaMallocHost`.\n",
    "\n",
    "Similar to kernel execution, `cudaMemcpyAsync` is only asynchronous by default with respect to the host. It executes, by default, in the default stream and therefore is a blocking operation with regard to other CUDA operations occuring on the GPU. The `cudaMemcpyAsync` function, however, takes as an optional 5th argument, a non-default stream. By passing it a non-default stream, the memory transfer can be concurrent to other CUDA operations occuring in other non-default streams.\n",
    "\n",
    "A common and useful pattern is to use a combination of pinned host memory, asynchronous memory copies in non-default streams, and kernel executions in non-default streams, to overlap memory transfers with kernel execution.\n",
    "\n",
    "In the following example, rather than wait for the entire memory copy to complete before beginning work on the kernel, segments of the required data are copied and worked on, with each copy/work segment running in its own non-default stream. Using this technique, work on parts of the data can begin while memory transfers for later segments occur concurrently. Extra care must be taken when using this technique to calculate segment-specific values for the number of operations, and the offset location inside arrays, as shown here:\n",
    "\n",
    "```cpp\n",
    "int N = 2<<24;\n",
    "int size = N * sizeof(int);\n",
    "\n",
    "int *host_array;\n",
    "int *device_array;\n",
    "\n",
    "cudaMallocHost(&host_array, size);               // Pinned host memory allocation.\n",
    "cudaMalloc(&device_array, size);                 // Allocation directly on the active GPU device.\n",
    "\n",
    "initializeData(host_array, N);                   // Assume this application needs to initialize on the host.\n",
    "\n",
    "const int numberOfSegments = 4;                  // This example demonstrates slicing the work into 4 segments.\n",
    "int segmentN = N / numberOfSegments;             // A value for a segment's worth of `N` is needed.\n",
    "size_t segmentSize = size / numberOfSegments;    // A value for a segment's worth of `size` is needed.\n",
    "\n",
    "// For each of the 4 segments...\n",
    "for (int i = 0; i < numberOfSegments; ++i)\n",
    "{\n",
    "  // Calculate the index where this particular segment should operate within the larger arrays.\n",
    "  segmentOffset = i * segmentN;\n",
    "\n",
    "  // Create a stream for this segment's worth of copy and work.\n",
    "  cudaStream_t stream;\n",
    "  cudaStreamCreate(&stream);\n",
    "  \n",
    "  // Asynchronously copy segment's worth of pinned host memory to device over non-default stream.\n",
    "  cudaMemcpyAsync(&device_array[segmentOffset],  // Take care to access correct location in array.\n",
    "                  &host_array[segmentOffset],    // Take care to access correct location in array.\n",
    "                  segmentSize,                   // Only copy a segment's worth of memory.\n",
    "                  cudaMemcpyHostToDevice,\n",
    "                  stream);                       // Provide optional argument for non-default stream.\n",
    "                  \n",
    "  // Execute segment's worth of work over same non-default stream as memory copy.\n",
    "  kernel<<<number_of_blocks, threads_per_block, 0, stream>>>(&device_array[segmentOffset], segmentN);\n",
    "  \n",
    "  // `cudaStreamDestroy` will return immediately (is non-blocking), but will not actually destroy stream until\n",
    "  // all stream operations are complete.\n",
    "  cudaStreamDestroy(stream);\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Overlap Kernel Execution and Memory Copy Back to Host\n",
    "\n",
    "The most recent iteration of the vector addition application, [01-manual-malloc-solution.cu](../edit/07-manual-malloc/solutions/01-manual-malloc-solution.cu), is currently performing all of its vector addition work on the GPU before copying the memory back to the host for verification.\n",
    "\n",
    "Refactor [01-manual-malloc-solution.cu](../edit/07-manual-malloc/solutions/01-manual-malloc-solution.cu) to perform the vector addition in 4 segments, in non-default streams, so that asynchronous memory copies can begin before waiting for all vector addition work to complete. Refer to [the solution](../edit/08-overlap-xfer/solutions/01-overlap-xfer-solution.cu) if you get stuck."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvcc -o vector-add-manual-alloc 07-manual-malloc/solutions/01-manual-malloc-solution.cu -run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After completing the refactor, open a report in Nsight Systems, and use the timeline to do the following:\n",
    "\n",
    "- Note when the device to host memory transfers begin, is it before or after all kernel work has completed?\n",
    "- Notice that the 4 memory copy segments themselves do not overlap. Even in separate non-default streams, only one memory transfer in a given direction (DtoH here) at a time can occur simultaneously. The performance gains here are in the ability to start the transfers earlier than otherwise, and it is not hard to imagine in an application where a less trivial amount of work was being done compared to a simple addition operation, that the memory copies would not only start earlier, but also overlap with kernel execution."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
